{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ATNN tutorial pt. 1: design of autograd functions\n",
    "\n",
    "## settings\n",
    "\n",
    "- ATNN only requires the [ATen library](https://github.com/zdevito/ATen) and recent C++17 compiler (GCC7, Clang4).\n",
    "- If you install pytorch via conda, you can find `${CONDA_PREFIX}/lib/python3.6/site-packages/torch/lib/libATen.so.1` because it is backend of pytorch.\n",
    "- If you did not have it or met any troubles, you can build it by `cd <atnn_repo>/test; make build/stage/lib/libATen.so`\n",
    "\n",
    "## concepts\n",
    "\n",
    "- Variable: autograd `at::Tensor` object defined at `<atnn/variable.hpp>`\n",
    "- Function: ephemeral forward/backward implementation object used inside Chain. As well as pytorch, Varibles are never directly applied to Functions. defined at `<atnn/function.hpp>`\n",
    "- Chain: autograd computation graph combining Functions and Variables defined at `<atnn/chain.hpp>`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    ".L libATen.so"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#include <atnn/function.hpp>\n",
    "#include <atnn/testing.hpp>\n",
    "#include <atnn/grad_check.hpp>\n",
    "#include <atnn/chain.hpp>\n",
    "#include <iostream>\n",
    "#include <vector>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IncrementalExecutor::executeFunction: symbol '__emutls_v._ZSt11__once_call' unresolved while linking [cling interface function]!\n",
      "IncrementalExecutor::executeFunction: symbol '__emutls_v._ZSt15__once_callable' unresolved while linking [cling interface function]!\n"
     ]
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// maybe cling's bug\n",
    "at::CPU(at::kFloat).randn({1})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functional interface for autograd function\n",
    "\n",
    "- use `atnn::chain::lambda(forward_func_obj, backward_func_obj)(args ...)` to create autograd functions. type of this expression is `atnn::Variable` or `std::vector<atnn::Variable>` that depends on the return type of `forwad_func_obj`, where\n",
    "  - `forward_func_obj` takes `std::vector<at::Tensor>` as inputs and returns `at::Tensor` or `std::vector<at::Tensor>`\n",
    "  - `backward_func_obj` takes `std::vector<atnn::Variable>` as inputs and returns `std::vector<atnn::Variable>`\n",
    "- you can find more examples in `atnn/chain.hpp`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// this function is double-backwardable\n",
    "auto fpow(atnn::Variable lhs, atnn::Variable rhs) {\n",
    "    return atnn::chain::lambda(\n",
    "        [](auto&& xs) { return xs[0].pow(xs[1]); },\n",
    "        [](auto&& xs, auto&& gys) { return\n",
    "            std::vector<atnn::Variable> {\n",
    "                gys[0] * fpow(xs[0], xs[1] - 1.0) * xs[1], // recursive call\n",
    "                gys[0] * fpow(xs[0], xs[1]) * xs[1].log()\n",
    "            };\n",
    "        }\n",
    "    )(lhs, rhs);\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable(\n",
      "data=\n",
      "  1   4\n",
      "  9  16\n",
      "[ CPUFloatTensor{2,2} ]\n",
      ")\n",
      "Variable(\n",
      "data=\n",
      " 2  4\n",
      " 6  8\n",
      "[ CPUFloatTensor{2,2} ]\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(std::basic_ostream<char, std::char_traits<char> >::__ostream_type &) @0x7f45d3f58420\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// functional pow test\n",
    "// if you use GCC, no need to write `atnn::Init<T, N>`\n",
    "atnn::Variable u0 = atnn::Init<float, 2> {{1, 2}, {3, 4}};\n",
    "auto u1 = fpow(u0, atnn::Init<float> {2});\n",
    "std::cout << u1 << std::endl;\n",
    "u1.backward();\n",
    "std::cout << u0.grad() << std::endl;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OOP interface for autograd function\n",
    "\n",
    "**NOTE** OOP style is not recommended because of its complexity. Use functional interface\n",
    "\n",
    "- use CRTP of `class Foo : atnn::function::Function<Foo>`\n",
    "- implement `auto Foo::impl_forward(const atnn::TList&)`. Here, return type is `at::Tensor` or `atnn::TList a.k.a. std::vector<at::Tensor>`\n",
    "- implement `atnn::VList Foo::impl_forward(const atnn::VList&)` to return each gradients w.r.t. inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// this function does not support double-backward\n",
    "struct Pow : atnn::function::Function<Pow> {\n",
    "    auto impl_forward(const std::vector<at::Tensor>& x) {\n",
    "        this->save_for_backward(x);\n",
    "        return x[0].pow(this->n);\n",
    "    }\n",
    "\n",
    "    std::vector<atnn::Variable> impl_backward(const std::vector<atnn::Variable>& gy) {\n",
    "        auto&& _x = this->saved_tensors[0];\n",
    "        return {atnn::Variable(gy[0].data() * _x.pow(this->n - 1) * this->n, false)};\n",
    "    }\n",
    "    double n = 2;\n",
    "    Pow(double n) : n(n) {}\n",
    "};"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable(\n",
      "data=\n",
      "  1   4\n",
      "  9  16\n",
      "[ CPUFloatTensor{2,2} ]\n",
      ")\n",
      "Variable(\n",
      "data=\n",
      " 2  4\n",
      " 6  8\n",
      "[ CPUFloatTensor{2,2} ]\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(std::basic_ostream<char, std::char_traits<char> >::__ostream_type &) @0x7f45d3f58420\n"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// oop pow test\n",
    "// if you use GCC, no need to write `atnn::Init<T, N>`\n",
    "atnn::Variable v0 = atnn::Init<float, 2> {{1, 2}, {3, 4}};\n",
    "auto func = atnn::chain::chain_ptr<Pow>(2); // need to wrap with chain_ptr\n",
    "auto v1 = func(v0);\n",
    "std::cout << v1 << std::endl;\n",
    "v1.backward();\n",
    "std::cout << v0.grad() << std::endl;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# gradient check\n",
    "\n",
    "you can use `atnn::grad_check` for validating your backward implementation with numeric grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(void) @0x7f45c9ff9c10\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "auto device = at::CPU;\n",
    "atnn::Variable x = device(at::kFloat).rand({3, 4});\n",
    "atnn::Variable y = device(at::kFloat).rand({3, 4});\n",
    "auto gy = device(at::kFloat).rand({3, 4});\n",
    "atnn::grad_check([](auto xs) { return fpow(xs[0], xs[1]); }, {x, y}, {gy});"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "C++17",
   "language": "C++",
   "name": "cling-cpp17"
  },
  "language_info": {
   "codemirror_mode": "c++",
   "file_extension": ".c++",
   "mimetype": "text/x-c++src",
   "name": "c++"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
